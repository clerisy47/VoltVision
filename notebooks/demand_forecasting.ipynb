{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand Forecasting: Regression Analysis and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problem Statement\n",
    "- The energy industry is undergoing a transformative journey, marked by rapid modernization and technological advancements. Infrastructure upgrades, integration of intermittent renewable energy sources, and evolving consumer demands are reshaping the sector. However, this progress comes with its challenges. Supply, demand, and prices are increasingly volatile, rendering the future less predictable. Moreover, the industry's traditional business models are being fundamentally challenged. In this competitive and dynamic landscape, accurate decision-making is pivotal. The industry relies heavily on probabilistic forecasts to navigate this uncertain future, making innovative and precise forecasting methods essential that aids stakeholders in making strategic decisions amidst the shifting energy landscape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Zenml-MLfow pipeline for Data Ingestion](../steps/demand_forecasting/data_ingestion.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Import Data and Required Packages\n",
    "- Importing Pandas, Numpy, Matplotlib, Seaborn, Scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge,Lasso\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Import the CSV Data as Pandas DataFrame\n",
    "- Importing both Demand and Weather Data of Demand Forecasting and merging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand = pd.read_csv('../dataset/Demand Forecasting/Demand Forecasting Demand Data upto Feb 21.csv', sep=',')\n",
    "df_weather = pd.read_csv('../dataset/Demand Forecasting/Demand Forecasting Weather Data upto Feb 28.csv', sep=',')\n",
    "df_merged=pd.merge(left=df_demand,right=df_weather, on='datetime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing and Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Zenml-MLfow pipeline for Data Preprocessing](../steps/demand_forecasting/data_preprocessing.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Show Top 5 Records\n",
    " - Showing top 5 and last 5 records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Checking if Unamed columns have any data\n",
    "- Checking the data in unnamed columns and removing all the empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(21, 26):\n",
    "    column_name = f'Unnamed: {i}'\n",
    "    count_non_null = df_merged[column_name].notna().sum()\n",
    "    print(f\"Non-null values in {column_name}: {count_non_null}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25']\n",
    "df_merged.drop(columns_to_drop, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Performing Datachecks\n",
    "- Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Filling most appropriate values for severerisk \n",
    "- Filling severerisk with 0 on nan values for more appropraite correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['severerisk'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Dropping redundant data\n",
    "- Dropping preciptype and precipprob as precipitation has more accurate and non-null data, similarly dropping windgust and keeping windspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop([\"precipprob\", \"preciptype\" ], inplace=True, axis=1)\n",
    "df_merged.drop(['windgust'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Interpolation of data\n",
    "- Using .interpolate() method to add most appropriate datas in place of NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_merged.columns[3:17]:\n",
    "    df_merged[column] = df_merged[column].interpolate(method='linear', limit_direction='forward', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Histogram & KDE\n",
    " - It is evident that the distribution of the 'Demand (MW)' column in the dataset closely aligns with a log-normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['datetime'] = pd.to_datetime(df_merged['datetime'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sns.histplot(df_merged['Demand (MW)'], kde=False, bins=30, color='skyblue', ax=ax)\n",
    "ax.set_title(f'Histogram of Demand (MW)')\n",
    "ax.set_xlabel('Demand (MW)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "sns.kdeplot(df_merged['Demand (MW)'], color='orange', ax=ax2)\n",
    "ax2.set_ylabel('KDE', color='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 Analyzing Correlation \n",
    "- Analyzing Correlation between Demand(MW) and other paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_merged.columns[3:18]:\n",
    "    print(f\"Correlation of Demand(MW) with {column}: {df_merged['Demand (MW)'].corr(df_merged[column])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotting a heatmap of corrlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = df_merged.columns[[1] + list(range(3, 10))]\n",
    "correlation_matrix =df_merged[selected_columns].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='Greens', fmt=\".2f\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 Regrestion Plot of highly correlated datas\n",
    "- Demand(MW) has high correlation with temperature and dewpoint whose regression plot are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='Temperature', y='Demand (MW)', data=df_merged, scatter_kws={'s': 10}, line_kws={'color': 'green'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='dewpoint', y='Demand (MW)', data=df_merged, scatter_kws={'s': 10}, line_kws={'color': 'green'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grouping data by month within each date and plotting a scatter plot between Temperature, dewpoint and Demand (MW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = df_merged\n",
    "tempdf['datetime'] = pd.to_datetime(df_merged['datetime'])\n",
    "\n",
    "numeric_columns = df_merged.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Grouping by date and then by month within each date, and calculating the mean for numeric columns\n",
    "monthlydf = tempdf.groupby(tempdf['datetime'].dt.to_period(\"M\"))[numeric_columns].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.regplot(x='Temperature', y='Demand (MW)', data=monthlydf, scatter_kws={'s': 10}, line_kws={'color': 'green'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='dewpoint', y='Demand (MW)', data=monthlydf, scatter_kws={'s': 10}, line_kws={'color': 'green'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10 Dropping less significant data\n",
    "Dropping less significant data after correlation analysis, i.e very low correlation as well as redundant data (i.e solarradiation and uv index where both have almost 1 correlation , here data with higher correlation is kept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged[\"solarradiation\"].corr(df_merged[\"uvindex\"]))\n",
    "print(df_merged[\"Temperature\"].corr(df_merged[\"feelslike\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(['feelslike','uvindex', 'precipitation', 'sealevelpressure', 'snow', 'snowdepth', 'windspeed', 'winddirection'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.11 Visualization of categorical data\n",
    "- Plotting conditions vs Demand(MW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x='conditions', y='Demand (MW)',hue='conditions', data=df_merged)\n",
    "\n",
    "plt.title('Conditions vs Demand (MW)')\n",
    "plt.xlabel('Conditions')\n",
    "plt.ylabel('Demand (MW)')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Zenml-MLfow pipeline for Feature Engineering](../steps/demand_forecasting/feature_engineering.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Handling Categorical Data\n",
    "Using pandas get dummies to handle categorical variables like \n",
    "condition creating new columns consisting of 0s and 1s for each columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df_merged['conditions'], prefix='overcast')\n",
    "df_final = pd.concat([df_merged, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.iloc[:,3:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Normalization\n",
    "- Normalize continuous values and avoid vanishing gradient problems to finalize our data before model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(df_merged.iloc[:,3:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "y = scaler.fit_transform(df_merged.iloc[:,1].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Train Validation Test Split\n",
    "- Splitting the data into different splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, train_size=0.8):\n",
    "    X_train = X[:int(train_size * len(X))]\n",
    "    X_test = X[int(train_size * len(X)):]\n",
    "    y_train = y[:int(train_size * len(y))]\n",
    "    y_test = y[int(train_size * len(y)):]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separating dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation\n",
    "#### [Zenml-MLfow pipeline for Model Evaluation](../steps/model_evaluaton/model_.py)\n",
    "#### 5.1 Create and Evaluate Function\n",
    "- Creating an evaulate function to give all metrics after model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true, predicted):\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(true, predicted))\n",
    "    r2_square = r2_score(true, predicted)\n",
    "    return mae, rmse, r2_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"K-Neighbors Regressor\": KNeighborsRegressor(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(),\n",
    "    \"XGBRegressor\": XGBRegressor(), \n",
    "    \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\n",
    "    \"AdaBoost Regressor\": AdaBoostRegressor()\n",
    "}\n",
    "model_list = []\n",
    "r2_list =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_grid_for_model(model_name):\n",
    "    if model_name == \"Lasso\":\n",
    "        return {'alpha': [0.1, 1.0, 10.0]}\n",
    "\n",
    "    elif model_name == \"Ridge\":\n",
    "        return {'alpha': [0.1, 1.0, 10.0]}\n",
    "\n",
    "    elif model_name == \"K-Neighbors Regressor\":\n",
    "        return {'n_neighbors': [3, 5, 7]}\n",
    "\n",
    "    elif model_name == \"Decision Tree\":\n",
    "        return {'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "    elif model_name == \"Random Forest Regressor\":\n",
    "        return {'n_estimators': [50, 100, 200], 'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "    elif model_name == \"XGBRegressor\":\n",
    "        return {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "\n",
    "    elif model_name == \"CatBoosting Regressor\":\n",
    "        return {'iterations': [50, 100, 200], 'depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "\n",
    "    elif model_name == \"AdaBoost Regressor\":\n",
    "        return {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Hyperparameter grid not defined for {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Hyperparameter Tuning\n",
    "- This script evaluates and compares the performance of various  models\n",
    "\n",
    "##### Models :\n",
    "- Lasso\n",
    "- Ridge\n",
    "- K-Neighbors Regressor\n",
    "- Decision Tree\n",
    "- Random Forest Regressor\n",
    "- XGBRegressor\n",
    "- CatBoosting Regressor\n",
    "- AdasBoost Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = {\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"K-Neighbors Regressor\": KNeighborsRegressor(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(),\n",
    "    \"XGBRegressor\": XGBRegressor(), \n",
    "    \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\n",
    "    \"AdaBoost Regressor\": AdaBoostRegressor()\n",
    "}\n",
    "\n",
    "model_list = []\n",
    "r2_list = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    param_grid = get_parameter_grid_for_model(model_name)\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2')\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    model_train_mae, model_train_rmse, model_train_r2 = evaluate_model(y_train, y_train_pred)\n",
    "    model_test_mae, model_test_rmse, model_test_r2 = evaluate_model(y_test, y_test_pred)\n",
    "\n",
    "    print(model_name)\n",
    "    print('Best hyperparameters:', grid_search.best_params_)\n",
    "    print('Model performance for Training set')\n",
    "    print(\"- Root Mean Squared Error: {:.4f}\".format(model_train_rmse))\n",
    "    print(\"- Mean Absolute Error: {:.4f}\".format(model_train_mae))\n",
    "    print(\"- R2 Score: {:.4f}\".format(model_train_r2))\n",
    "    print('----------------------------------')\n",
    "    print('Model performance for Test set')\n",
    "    print(\"- Root Mean Squared Error: {:.4f}\".format(model_test_rmse))\n",
    "    print(\"- Mean Absolute Error: {:.4f}\".format(model_test_mae))\n",
    "    print(\"- R2 Score: {:.4f}\".format(model_test_r2))\n",
    "    r2_list.append(model_test_r2)\n",
    "    print('=' * 35)\n",
    "    print('\\n')\n",
    "\n",
    "    model_list.append(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Training\n",
    "#### [Zenml-MLfow pipeline for Data Preprocessing](../steps/demand_forecasting/model_training.py)\n",
    "\n",
    "#### 6.1 XGBoost Regression Model Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(xgb_model, '../model/demand_forecasting.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Evaluation\n",
    " After training the XGBoost regression model for demand forecasting, following metrics are used to assess the performance : \n",
    "\n",
    " - Mean Squared Error \n",
    " - Mean Absolute Error\n",
    " - R2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean squared error:\", mean_squared_error(y_test, xgb_pred))\n",
    "print(\"mean absolute error:\",mean_absolute_error(y_test, xgb_pred))\n",
    "print(\"r2 score:\",r2_score(y_test, xgb_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
